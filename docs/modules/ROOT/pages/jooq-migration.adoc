= Developer-centric SQL Migrations

:toc:

Bei vielen Arten von Migrationsprojekten steht man vor der Herausforderung der Datenmigration.
In diesem Artikel wird gezeigt, wie jOOQ dabei unterstützen kann, sowohl Änderungen am Source Schema als auch am Target Schema zu erkennen, (gewisse) Fehler durch Automatisierung zu vermeiden und Änderungen aufgrund von Compile-Time-Safety zu verifizieren.

== Ausgangssituation

Die hier beschriebene Situation haben wir schon in ähnlicher Art in einigen Migrationsprojekten erlebt.
Das Quellsystem war eine Oracle Datenbank, das Zielsystem eine PostgreSQL Datenbank.
Dabei wurde die Oracle Datenbank im Wesentlichen als Datensenke betrachtet und enthielt keine Geschäftslogik.

Die Datenbank wurde mittels ora2pg in ein PostgreSQL "Staging" Schema migriert.
Das Schema halten wir in der gleichen Datenbank, wie das spätere Zielschema.

Das haben wir gemacht, damit wir einfache SQL Abfragen zur Migration nutzen können.
Es kann aber auch sinnvoll sein, das Staging Schema in einer eigenen Datenbank, bzw.
Datenbankcluster zu halten, um die Datenbanken zu entkoppeln.
Dann kann mann Postgres Foreign Data Wrapper nutzen, um genauso einfach auf die Daten zuzugreifen.
Dies ist sinnvoll, da bei vielen Cloudanbietern, der einmal zugewiesene Speicher nicht wieder einfach freigegeben werden kann.
Liegt das Staging Schema in einem eigenen Datnbankcluster, so kann der gesamte Server einfach gelöscht werden.

Wir halten es für sinnvoll möglichst frühzeitig mit der Entwicklung der Migrationsskripte zu beginnen.
Häufig stellt man fest, dass die Altdaten Konstellationen enthalten, die man in Zukunft nicht mehr zulassen möchte.
Dann stellt sich die Frage, wie mit den Altdaten umgegangen werden soll.

Gleichzeitig findet aber die Migration oder Neuimplementierung der Anwendung statt.
Die Migration als Treiber sorgt für einen Datenbank-First Ansatz.
Ebenfalls hilfreich ist es, dass man mit den Altdaten arbeiten kann.
Dies liefert eine realistische Datenmenge, denn leere Datenbanken sind immer schnell.
Das Thema Nutzung von Produktionsdaten zu Testzwecken ist aber ein anderes Thema.

Wenn sich aber das Zielschema noch ändert, so müssen die Migrationsskripte angepasst werden.
Migrationsskripte sind aber von den Liquibase oder Flyway Skripten getrennt.

== Was gehört zu einer performanten Migration?

Es gibt verschiedene Arten von Migrationen.
In der Regel ist es sinnvoll bei IT Systemen eine inkrementelle Migrationsstrategie zu verfolgen.
Das kann trotzdem bedeuten, dass Teile des IT-Systems jeweils in einem Schritt migriert werden (Big Bang).
Dann ist häufig eine performante Migration der Daten wichtig.

Beim Bulk load von Daten in eine Postgres Datenbank oder zwischen Tabellen gibt es dazu einiges zu beachten.
Ähnliches gilt auch für andere Datenbanken.

Angelehnt an die Liste von https://www.enterprisedb.com/blog/7-best-practice-tips-postgresql-bulk-data-loading[EDB] sind folgende Punkte wichtig für unser Szenario (Postgres -> Postgres mittels SQL):

* Tabellen in die Migriert werden sollen, sollten vorher leer sein.
* Tabellen sollten "unlogged" sein.
* Tabellen sollten keine Indizes haben.
* Tabellen sollten keine Constraints haben.
* Tabellen sollten keine Triggers haben.
* Tabellen sollten keine Foreign Keys haben.

Dann werden die Daten mittels SQL Skripten geladen.
Und dann werden die Indizes, Constraints, Triggers und Foreign Keys wiederhergestellt.
Zum Abschluss sollte ein Analyze ausgeführt werden.

== Was ist die Herausforderung bei gleichzeitiger Entwicklungstätigkeiten?

Wird die Anwendung neu entwickelt, so ändert sich das Zielschema.
Dabei sind insbesondere die Änderungen an den Zieltabellen interessant.

* Neue Indizes
* Neue Constraints (inklusive Foreign Keys)
* Neue Triggers
* Umbenennungen von Spalten
* Neue Not Null Spalten

Es ist wichtig, dass diese Änderungen entweder automatisch erkannt und berücksichtigt werden oder vor Ausführung der Skripte auffallen.
Da die Laufzeiten für die Migrationen sehr lang sein können (Stunden bis Tage), ist es wichtig, dass die Migrationsskripte möglichst fehlerfrei sind.
Ansonsten ärgert man sich über einfache Fehler, die früher hätten erkannt werden können und weil die Probemigration nicht abgeschlossen werden konnte.

== Wie kann jOOQ helfen?

jOOQ generiert aus einem Datenbankschema Klassenmodell.
Dies beinhaltet Informationen über die gängigen Datenbankobjekte wie Tabellen, Spalten, Indizes, Constraints, Triggers und Foreign Keys.
Mittels dieses Klassenmodells kann man dann einfach alle Indizes oder Constraints einer Tabelle entfernen oder neu erzeugen.
Wenn wir dabei alle Tabellen mittels eines https://en.wikipedia.org/wiki/Topological_sorting[topologischen Sortieralgorithmus] sortieren, dann können wir die Tabellen in der richtigen Reihenfolge behandeln.
Dies gilt für diesen Blog Post zumindest solange der "Tabellengraph" keine Zyklen enthält.
Damit vergessen wir keine Objekte zu löschen oder zu erzeugen, die im Rahmen der Weiterentwicklung noch entstehen oder gelöscht werden.
Statt die Änderungen direkt anzuwenden, können wir uns das SQL ausgeben lassen und einsammeln.
Im Beispiel gibt es aktuell keinen Schalter, die Skripte auch direkt gegen die Datenbank auszuführen.
Generell sollten die Aktionen nicht vom "Entwicklerlaptop" ausgeführt werden, sondern von einem anderen System.
Wenn man die Migration manuell startet, sollte man darauf achten, dass dies unter einem terminal multiplexer wie tmux oder screen geschieht, damit die Verbindung zur Datenbank nicht abbricht.
Die Migrationsdauer hängt dabei stark von der Größe der Datenbank ab.
Selbst für einige kleine Datenbanken mit < 250 Mio Einträgen und mehreren Tabellen kann die Migration mehrere Stunden dauern.
Bei größeren Migrationen ist sicherlich nochmal eine gesonderte Betrachtung notwendig, um Laufzeiten zu optimieren.

== Das Beispielprojekt

Das Beispielprojekt ist auf https://github.com/opitzconsulting/jooq-migration[GitHub] verfügbar.
Eine etwas ausführlichere Dokumentation findet sich auf den zugehörigen https://opitzconsulting.github.io/jooq-migration/jooq-migration/main/index.html[Github Pages].

=== Datenbank aufsetzen

Die Postgres Datenbank kann mittels Docker Compose aufgesetzt werden.

[source,bash]
----
docker compose up -d
----

Verbinden kann man sich mittels des Nutzers jooq_demo_admin und dem Passwort jooq_demo_admin.
Dieser Nutzer hat die notwendigen Rechte auf der Datenbank und den Schemas.
Es gibt die folgenden Schemas:

jooq_demo::
Das Zielschema, welches später von der Anwendung genutzt werden soll
staging::
Das Staging Schema, welches aus der Oracle Datenbank migriert wurde
extensions::
Das Schema, in dem die Erweiterungen für die Datenbank liegen (z.B. uuid-ossp), welche nicht zu viele eigene Funktionen / Objekte bereitstellen

Wenn die Datenbank läuft, dann können die Tabellen mittels

[source,bash]
----
./gradlew :db:update
----

erzeugt werden.
Die Ausführung von liquibaseUpdate triggert auch die Erzeugung der jOOQ Klassen.

Es wird eine minimale Web Oberfläche mittels https://www.adminer.org[Adminer] bereitgestellt, die unter http://localhost:8080 erreichbar ist.
Wer sich darüber anmelden möchte, der kann die folgenden Daten verwenden:

|===
| Attribut | Wert
| Datenbank System | PostgreSQL
| Server | postgres
| Benutzer | jooq_demo_admin
| Passwort | jooq_demo_admin
| Datenbank | jooq_demo
|===

=== Spring Shell bauen

Die Anwendung, welche die Testdaten generiert, Migrationsskripte erstellt und ausführt, ist eine Spring Shell Anwendung.

Ein Gradle build erzeugt die Anwendung.
[source,bash]
----
./gradlew build
----

=== Testdaten erzeugen

Jetzt benötigen wir Testdaten in der Staging Datenbank.
Die Daten werden mittels http://datafaker.net[DataFaker] erzeugt und mittels jOOQ in die Datenbank geschrieben.

[source,bash]
----
java -jar library-migration/build/libs/library-migration-0.0.1-SNAPSHOT.jar generateData
----

=== Migrationsskripte erstellen

Jetzt geht es darum, die eigentlichen Migrationsskripte erstellen zu lassen.

[source,bash]
----
java -jar library-migration/build/libs/library-migration-0.0.1-SNAPSHOT.jar generateScripts
----

Die Migrationsskripte werden im `scripts` Verzeichnis des Sub-Projekts `library-migration` abgelegt.
Es wird ein Skript `0000_run_all.sql` erzeugt, welches mittels `psql` ausgeführt werden kann.

Dieser Schritt enthält die eigentliche Magie.

Wenn wir jetzt einmal die Skripte anschauen, die erzeugt werden, so sehen wir folgende Liste:

[source,bash]
----
0000_run_all.sql
0010_disable_indexe.sql
0020_drop_constraints.sql
0030_unlog_tables.sql
1010_create_mapping_tables.sql
1020_members.sql
1030_books.sql
1040_checkout.sql
2040_log_tables.sql
2050_add_constraints.sql
2060_enable_indexe.sql
2070_analyze_tables.sql
----

Einige Dateien sind aktuell leer, wie disable_indexe und enable_indexe, da keine expliziten Indexe definiert sind.
Wir erkennen aber alle Schritte wieder, die wir im Rahmen eines Builds durchführen würden.

Wir brauchen aber nur die Skripte schreiben, welche die wirkliche Migration der Daten durchführen.

.Book Mapping using a lookup table
[source,java]
----
   var sql = dsl.insertInto(
                            target,
                            target.ID,
                            target.INSTANCE_ID,
                            target.MEMBER_ID,
                            target.CHECKOUT_DATE,
                            target.RETURN_DATE,
                            target.ACTUAL_RETURN_DATE)
                    .select(dsl.select(
                                    Routines.uuidGenerateV7(),
                                    instance.ID,
                                    mappingMembers.UUID,
                                    source.CHECKOUT_DATE,
                                    source.RETURN_DATE,
                                    source.ACTUAL_RETURN_DATE)
                            .from(source)
                            .join(mappingBooks)
                            .on(source.ISBN13.eq(mappingBooks.ISBN13))
                            .join(mappingMembers)
                            .on(source.MEMBER_ID.eq(mappingMembers.MEMBER_ID))
                            .join(instance)
                            .on(instance.BOOK_ID.eq(mappingBooks.UUID)))
                    .onConflictDoNothing()
                    .getSQL();
----

Dieses Beispiel befindet sich in der Klasse `LibraryMigratio`, welche die Methode `migrateTables`  unserer `FullMigrationSupport` Oberklasse überschreibt.

.Überschriebene Methode `migrateTables`
[source,java]
----
  @Override
    protected void migrateTables() throws Exception {
        createBookMappingTables(migrationScriptsCollector.newScript("1010_create_mapping_tables.sql"));
        mapMembers(migrationScriptsCollector.newScript("1020_members.sql"));
        mapBooks(migrationScriptsCollector.newScript("1030_books.sql"));
        mapCheckouts(migrationScriptsCollector.newScript("1040_checkout.sql"));
    }
----

=== Migrationsskripte ausführen

Jetzt müssen die Daten mittels der Skripte migriert werden.

[source,bash]
----
java -jar library-migration/build/libs/library-migration-0.0.1-SNAPSHOT.jar applyScripts
----

Das führt einfach die Skripte aus (mittels Springs ScriptUtils Klasse).

=== Zusätzlichen Index erstellen

Wir wollen einen zusätzlichen Index auf der Tabelle `book` erstellen.
Dieser sollte dann auch automatisch in den Migrationsskripten gedroppt und neu erstellt werden.

[source,bash]
----
./gradlew :db:update -PliquibaseExtraArgs="contexts=demo-1"
./gradlew --no-build-cache clean build
java -jar library-migration/build/libs/library-migration-0.0.1-SNAPSHOT.jar generateScripts
----

Das Skript `0010_disable_index.sql` enthält jetzt den Befehl zum Löschen des Indexes und entsprechend `2060_enable_indexe.sql` enthält den Befehl zum Erstellen des Indexes.

Es gibt also keine Indexe, die man vergessen kann zu löschen und zu erstellen.

Das ist schon mal sehr hilfreich.
Was hier passiert ist, dass an ein liquibase Update (mittels `./gradlew :db:update`) den Task `jooqCodegen` triggert.
Damit wird das jOOQ Klassenmodell aktualisiert.

Wir schauen uns jetzt einmal an, was passiert, wenn wir eine Spalte umbenennen.

=== Spalte umbenennen

Wir nennen in der Tabelle `checkout`die Spalte `return_date` in `borrowed_until_date` um.

[source,bash]
----
./gradlew :db:update -PliquibaseExtraArgs="contexts=demo-2"
./gradlew --no-build-cache clean build
----

Der Build schlägt fehl, da es die Spalte mit dem alten Namen nicht mehr gibt.

.Fehlermeldung
[source,bash]
----
/Users/rat/devel/playground/jooq-migration/library-migration/src/main/java/com/opitzconsulting/cattlecrew/jooqmigration/migration/LibraryMigration.java:90: Fehler: Symbol nicht gefunden
                            target.RETURN_DATE,
----

Da es nur ein Demo ist, können wir die Spalte wieder umbenennen und den Build wiederholen.

[source,bash]
----
./gradlew :db:rollbackCount -PliquibaseCommandValue=1 -PliquibaseExtraArgs="contexts=demo-2"
----

Wer noch mehr ausprobieren möchte, der kann das natürlich auch einfach mittels SQL auf der Datenbank machen.

== Was nicht funktioniert

Wenn man eine neue Not Null Spalte hinzufügt, dann wird das nicht automatisch erkannt, dass in den Abfragen kein Wert für die Spalte bereitgestellt wird.
Wer die Datenbankdefinition sich genau angesehen hat, wird feststellen, dass ein Postgres spezifisches Exclusion constraint auf der Checkout Tabelle definiert ist.
Dieses wurde vom Meta-Modell nicht erkannt und auch nicht in den Skripten berücksichtigt.
Daher ist es immer noch wichtig, sich die generierten Skripte anzuschauen und ggf. zu prüfen und zu ergänzen.
Gleiches gilt auch, wenn zyklische Abhängigkeiten zwischen Tabellen bestehen.

== Fazit

Auch wenn nicht Änderungen erkannt werden, ist die Nutzung von jOOQ für die Erstellung von Migrationsskripten sehr hilfreich und kann einige Fehlerquellen adressieren, die bei gleichzeitiger Entwicklungstätigkeit auftreten können, wenn also das Zielschema nicht eingefroren ist.
Notwendige Ergänzungen können im Code oder in den generierten Skripten vorgenommen werden.
